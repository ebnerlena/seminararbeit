@article{Butscher2018,
abstract = {Immersive technologies such as augmented reality devices are opening up a new design space for the visual analysis of data. This paper studies the potential of an augmented reality environment for the purpose of collaborative analysis of multidimensional, abstract data. We present ART, a collaborative analysis tool to visualize multidimensional data in augmented reality using an interactive, 3D parallel coordinates visualization. The visualization is anchored to a touch-sensitive tabletop, benefiting from well-established interaction techniques. The results of group-based expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. Based on the results, we provide a set of guidelines and discuss future research areas to foster the development of immersive technologies as tools for the collaborative analysis of multidimensional data.},
annote = {Interactive visualization is increasing as there is a rapid development of new immersive displays and input technologies. They provide the means to visualize complex information in a physical space, allowing a large amount of data to be investigated simultaneously. and in a more natural way. Typical aims of analysisa re identificationof high-dimensional clusters and their correlation to related outcomes, investigation of high-dimensional data on mulitple aggregation levels, analis of chronological trends within data and identification of outlier in the data. Tolls for these visualizations need functionalities such as filtering and clustering the data, or adding, removing and rearranging the dimensions. ART, a collaborative analysis tool to visualize multidimensional data in AR using interactive, 3D parallel coordinates visualization. Visualization is anchored to a touch-sensitve tabletop.},
author = {Butscher, Simon and Hubenschmid, Sebastian and M{\"{u}}ller, Jens and Fuchs, Johannes and Reiterer, Harald},
doi = {10.1145/3173574.3173664},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/Clusters_Trends_and_Outliers_How_Immersive_Technol.pdf:pdf},
isbn = {9781450356206},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {3D parallel coordinates,Augmented reality,Collaboration,Immersive analytics,Multi-touch table},
number = {April},
title = {{Clusters, trends, and outliers: How Immersive technologies can facilitate the collaborative analysis of multidimensional data}},
volume = {2018-April},
year = {2018}
}
@article{Billinghurst2008,
abstract = {This paper advocates a new metaphor for designing threedimensional Augmented Reality (AR) applications, Tangible Augmented Reality (Tangible AR). Tangible AR interfaces combine the enhanced display possibilities of AR with the intuitive manipulation and interaction of physical objects or Tangible User Interfaces. We define what Tangible AR interfaces are, present some design guidelines and prototype interfaces based on these guidelines. Experiences with these interfaces show that the Tangible AR metaphor supports seamless interaction between the real and virtual worlds, and provides a range of natural interactions that are difficult to find in other AR interfaces.},
annote = {Tangible AR interfaces support seamless interaction between real and virtual worlds by providing a range of natural interactions - more intuititve. Great advances in AR display and tracking techniques but interaction is limited and only few applications let users interact, request and modify data effectivly and in real time. Tangible AR interfaces register each virtual object to a physical object. Inspired by Tangible Media group real world objects are used as computer input and output devices - coupling an AR visual display to a tangible physical interace.},
author = {Billinghurst, Mark and Kato, Hirokazu and Poupyrev, Ivan},
doi = {10.1145/1508044.1508051},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/TangibleAugmentedReality.pdf:pdf},
journal = {ACM SIGGRAPH ASIA 2008 Courses, SIGGRAPH Asia'08},
keywords = {Augmented reality,CSCW,Collaboration,Tangible user interfaces},
title = {{Tangible augmented reality}},
year = {2008}
}
@article{Besancon2017,
abstract = {We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined - focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.},
annote = {Exploring the design space for combining tactile and tangible inputs in 3d data visualization and interaction by hybrid mapping for common 3D visualization tasks, as research shows that both tactile and tangible input have many benefits for effective and efficient 3D data exploration interaction. Using Google's Tango tablet as portable device. Participants found prototype better than traditional mouse-keyboard setup.},
author = {Besan{\c{c}}on, Lonni and Issartel, Paul and Ammi, Mehdi and Isenberg, Tobias},
doi = {10.1109/TVCG.2016.2599217},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/HybridTactile_TangibleInteractionFor3DDataExploration.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D data visualization,Interaction,tactile input,tangible input},
number = {1},
pages = {881--890},
pmid = {27875202},
title = {{Hybrid Tactile/Tangible Interaction for 3D Data Exploration}},
volume = {23},
year = {2017}
}
@article{Yang2021,
abstract = {Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants' response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.},
annote = {Abstract data is not based in a phyiscal reference space and can be freely re-scaled an dviewed form any angle. In a desktop data visualization setup typical interactions are zoom+pan and overview+detail. In immersive environments there is teleportation for navigating in a space thath is to large to fully explore by physical movement. Equivalent to desktop minimap overview+detail in VR is Wolrd In Miniature (WIM) navigation. Also zooming is possible in immersive environments, to scale the inforamtion space around the user.},
archivePrefix = {arXiv},
arxivId = {2008.09941},
author = {Yang, Yalong and Cordeil, Maxime and Beyer, Johanna and Dwyer, Tim and Marriott, Kim and Pfister, Hanspeter},
doi = {10.1109/TVCG.2020.3030427},
eprint = {2008.09941},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/EmbodiedNavigationInImmersiveAbstractDataVisualisation.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Immersive Analytics,Information Visualization,Navigation,Overview+Detail,Scatterplot,Virtual Reality,Zooming},
number = {2},
pages = {1214--1224},
pmid = {33048730},
title = {{Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?}},
volume = {27},
year = {2021}
}
@article{Buschel2017,
abstract = {Three-dimensional visualizations employing traditional input and output technologies have well-known limitations. Immersive technologies, natural interaction techniques, and recent developments in data physicalization may help to overcome these issues. In this context, we are specifically interested in the usage of spatial interaction with mobile devices for improved 3D visualizations. To contribute to a better understanding of this interaction style, we implemented example visualizations on a spatially-tracked tablet and investigated their usage and potential. In this paper, we report on a qualitative study comparing spatial interaction with in-place 3D visualizations to classic touch interaction regarding typical visualization tasks: navigation of unknown datasets, comparison of individual data objects, and the understanding and memorization of structures in the data. We identify several distinct usage patterns and derive recommendations for using spatial interaction in 3D data visualization.},
annote = {Occlusion, misleading perspective and poor readability are limiting the 3D fluid dynamics visualization. Immersive Analytics is in need of effective interaction techniques to be more natural, tangible and engaging.  Mobile devices with their mulit-touch input have become an increasingly important platform for such visualizations. Spacial interaction is an alternative to touch input and may help lift visualizations form surfaces into interactie spaces. Therefor the movement of handheld, spartially-ware mobile devices is used to interact with data virtually residing in physical space.},
author = {B{\"{u}}schel, Wolfgang and Reipschl{\"{a}}ger, Patrick and Langner, Ricardo and Dachselt, Raimund},
doi = {10.1145/3132272.3134125},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/InvestigatingTheUseOfSpatialInteractionFor3DDataVisualizationOnMobileDevices.pdf:pdf},
isbn = {9781450346917},
journal = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces, ISS 2017},
keywords = {3D Data Visualization,Immersive Visualization,Mobile Devices,Spatial Input,Tangible Displays},
pages = {62--71},
title = {{Investigating the use of spatial interaction for 3D data visualization on mobile devices}},
year = {2017}
}
@article{Cordeil2017,
abstract = {We introduce the concept of 'spatio-data coordination' (SD coordination) which defines the mapping of user actions in physical space into the space of data in a visualisation. SD coordination is intended to lower the user's cognitive load when exploring complex multi-dimensional data such as biomedical data, multiple data attributes vs time in a space-time-cube visualisation, or three-dimensional projections of three-or-higher-dimensional data sets. To inform the design of interaction devices to allow for SD coordination, we define a design space and demonstrate it with sketches and early prototypes of three exemplar devices for SD coordinated interaction.},
annote = {SD coordination is intended to lower the users cognitive load when exploring complex mutlidimensional data. Spatial position is the most effective channel for mapping a quantitive data attribute to a visual representation. Products with high-resolution displays and precise low-latency head-motion tracking together with sterovision and kinestetic depth greatly improve human perception of 3D space. Accurate head-tracking enables embodied navigation in a visualisation. Aligning the dimensions of the data with the affordances of devices used to interact.},
author = {Cordeil, Maxime and Bach, Benjamin and Li, Yongchao and Wilson, Elliott and Dwyer, Tim},
doi = {10.1109/PACIFICVIS.2017.8031578},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/DesignSpaceForSpatio-Data_Coordination_TangibleInteractionDevicesForImmersiveInformationVisualisation.pdf:pdf},
isbn = {9781509057382},
issn = {21658773},
journal = {IEEE Pacific Visualization Symposium},
pages = {46--50},
title = {{Design space for spatio-data coordination: Tangible interaction devices for immersive information visualisation}},
volume = {2017},
year = {2017}
}
@article{Cordeil2019,
abstract = {We introduce IATK, the Immersive Analytics Toolkit, a software package for Unity that allows interactive authoring and exploration of data visualisation in immersive environments. The design of IATK was informed by interdisciplinary expert-collaborations as well as visual analytics applications and iterative refinement over several years. IATK allows for easy assembly of visualisations through a grammar of graphics that a user can configure in a GUI-in addition to a dedicated visualisation API that supports the creation of novel immersive visualisation designs and interactions. IATK is designed with scalability in mind, allowing visualisation and fluid responsive interactions in the order of several million points at a usable frame rate. This paper outlines our design requirements, IATK's framework design and technical features, its user interface, as well as application examples.},
annote = {IATK - an open-source Immersive Analytics Toolkit, a software package for Unity that allows interactive and exploration of data visualisation in immersive environments. Assembly of visualisations happens through a grammar of graphics that the user can configure in a GUI. Additional there is a dedeicated visualisation API.  Unity has become a standard plattform for immersive environments. IATK is scalable and addresses the gap of providing standard routines, predefined visualisations and support for creating novel visualisations and interaction.},
author = {Cordeil, Maxime and Cunningham, Andrew and Bach, Benjamin and Hurter, Christophe and Thomas, Bruce H. and Marriott, Kim and Dwyer, Tim},
doi = {10.1109/VR.2019.8797978},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/IATK_Immersive_Analytics_Toolkit.pdf:pdf},
isbn = {9781728113777},
journal = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
keywords = {Centered computing,Human,Visualisation design and evaluation methods,Visualization,Visualization techniques},
pages = {200--209},
title = {{IATK: An immersive analytics toolkit}},
year = {2019}
}
@article{Prouzeau2019,
abstract = {Three-dimensional scatterplots suffer from well-known perception and usability problems. In particular, overplotting and occlusion, mainly due to density and noise, prevent users from properly perceiving the data. Thanks to accurate head and hand tracking, immersive Virtual Reality (VR) setups provide new ways to interact and navigate with 3D scatterplots. VR also supports additional sensory modalities such as haptic feedback. Inspired by methods commonly used in Scientific Visualisation to visually explore volumes, we propose two techniques that leverage the immersive aspects of VR: first, a density-based haptic vibration technique (Scaptics) which provides feedback through the controller; and second, an adaptation of a cutting plane for 3D scatterplots (Highlight-Plane). We evaluated both techniques in a controlled study with two tasks involving density (finding high- and low-density areas). Overall, Scaptics was the most time-efficient and accurate technique, however, in some conditions, it was outperformed by Highlight-Plane.},
annote = {Scatterplots offer a direct mapping from the attribute to 2D position in a plane orthogonal to the viewer. 3D scatterplots include three quantitve attributes and allow to visualise these attributes, projetions of higher-dimensional space or spatial data. Typical issues include occlusion and perspective distortion. Highlight Plane is a redesigned cutting-plane technique asstraightfoward adaoption of a cutting plane is inappropriate for scatterplot data. The vibrotactile feedback in controllers supports user understanding of occluded local scatterplot density and is using KDE(Kernel Density Estimation), called Scaptics.},
author = {Prouzeau, Arnaud and Cordeil, Maxime and Robin, Clement and Ens, Barrett and Thomas, Bruce H. and Dwyer, Tim},
doi = {10.1145/3290605.3300555},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/Scaptics_and_Highlight_PlaneImmersiveInteractionTechniques.pdf:pdf},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {3D scatterplot,Haptic,Vibrotactile feedback,Virtual reality},
title = {{Scaptics and highlight-planes: Immersive interaction techniques for finding occluded features in 3D scatterplots}},
year = {2019}
}
@article{Sicat2019,
abstract = {This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.},
annote = {DXR, an open-source toolkit for building immersive data visualizations based on Unity. As immserive visualizations often requires domain knowledge such as low-level programming, this can hinder the iterative ieda-to-prototype process especially for developer without this knowledge. With DXR developers can build immersive visualizatio using consice decaltrative grammar inspired by Vega-Lite, a GUI for easy and quick edits and reusable templates. DXR vsualizations are Unity GameObjects which allow all Unity feature e.g. object tracking, styling, placement..},
author = {Sicat, Ronell and Li, Jiabao and Choi, Junyoung and Cordeil, Maxime and Jeong, Won Ki and Bach, Benjamin and Pfister, Hanspeter},
doi = {10.1109/TVCG.2018.2865152},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/DXR_Toolkit_for_Building_Immersive_Data_Visualizations.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented Reality,Immersive Analytics,Immersive Visualization,Virtual Reality,Visualization Toolkit},
number = {1},
pages = {715--725},
pmid = {30136991},
title = {{DXR: A Toolkit for Building Immersive Data Visualizations}},
volume = {25},
year = {2019}
}
@article{Drogemuller2020,
abstract = {Research into how virtual reality (VR) can be a beneficial technology for new and emerging large, complex data visualisations for data scientists is ongoing. In this paper, we evaluate three-dimensional VR navigation technique for data visualisations and test their effectiveness with a large graph visualisation. We evaluate two prominent navigation techniques employed in VR (Teleportation and One-Handed Flying) against two less common methods (Two-Handed Flying and Worlds-In-Miniature) and evaluate their performance and effectiveness through a series of tasks. We found Steering Patterns (One-Handed Flying and Two-Handed Flying) to be faster and preferred by participants for completing searching tasks in comparison to Teleportation. Worlds-In-Miniature was the least physically demanding of the navigations, and was preferred by participants for tasks that required an overview of the graph such as triangle counting.},
annote = {Because of limitation when exploring and interacting with big datasets on desktop setup, reseacher explore new technologies inlcudng CAVE and Head-mounted displays. Vrige - a immersive graph explorer - to explore the increased immersion and sense of presence in VR. To navigate large graphs in room-sized VR, walking to a point can be insufficient due to limitation of the room size. Other interactive VR navigation techniques such as Teleportation, One-handed flying, two-handed flying and Wolrd-in-Miniature are explored.},
author = {Drogemuller, Adam and Cunningham, Andrew and Walsh, James and Thomas, Bruce H. and Cordeil, Maxime and Ross, William},
doi = {10.1016/j.cola.2019.100937},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/ExaminingVirtualRealityNavigationTechniquesFor3DNetworkVisualisations.pdf:pdf},
issn = {25901184},
journal = {Journal of Computer Languages},
keywords = {Graphs,Navigation,Virtual reality,Visualisation},
number = {December 2019},
pages = {100937},
publisher = {Elsevier},
title = {{Examining virtual reality navigation techniques for 3D network visualisations}},
url = {https://doi.org/10.1016/j.cola.2019.100937},
volume = {56},
year = {2020}
}
@article{Yi2007,
abstract = {Even though interaction is an important part of information visualization (Infovis), it has garnered a relatively low level of attention from the Infovis community. A few frameworks and taxonomies of Infovis interaction techniques exist, but they typically focus on low-level operations and do not address the variety of benefits interaction provides. After conducting an extensive review of Infovis systems and their interactive capabilities, we propose seven general categories of interaction techniques widely used in Infovis: 1) Select, 2) Explore, 3) Reconfigure, 4) Encode, 5) Abstract/Elaborate, 6) Filter, and 7) Connect. These categories are organized around a user's intent while interacting with a system rather than the low-level interaction techniques provided by a system. The categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science of interaction. {\textcopyright} 2007 IEEE.},
author = {Yi, Ji Soo and Kang, Youn Ah and Stasko, John T. and Jacko, Julie A.},
doi = {10.1109/TVCG.2007.70515},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/TowardADeeperUnderstandingOfTheRoleOfInteractionInInformationVisualization.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Information visualization,Interaction,Interaction techniques,Taxonomy,Visual analytics},
number = {6},
pages = {1224--1231},
publisher = {IEEE},
title = {{Toward a deeper understanding of the role of interaction in information visualization}},
volume = {13},
year = {2007}
}
@article{Wang2020,
abstract = {We present an observational study with domain experts to understand how augmented reality (AR) extensions to traditional PC-based data analysis tools can help particle physicists to explore and understand 3D data. Our goal is to allow researchers to integrate stereoscopic AR-based visual representations and interaction techniques into their tools, and thus ultimately to increase the adoption of modern immersive analytics techniques in existing data analysis workflows. We use Microsoft's HoloLens as a lightweight and easily maintainable AR headset and replicate existing visualization and interaction capabilities on both the PC and the AR view. We treat the AR headset as a second yet stereoscopic screen, allowing researchers to study their data in a connected multi-view manner. Our results indicate that our collaborating physicists appreciate a hybrid data exploration setup with an interactive AR extension to improve their understanding of particle collision events.},
annote = {A study about AR extensions help scientists and physicists to explore and understand 3D data such as particle collisions. Using the Microsoft HoloLens as AR headset and mouse input as traditional input device for data analysis. AR compared to VR also offers immersive 3D steroscopic data view, allowing them to interact with real-world objects.},
author = {Wang, Xiyao and Besan{\c{c}}on, Lonni and Rousseau, David and Sereno, Mickael and Ammi, Mehdi and Isenberg, Tobias},
doi = {10.1145/3313831.3376657},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/TowardsAnUnderstandingOfAugmentedRealityExtensionsForExisting3DAnalysisTools.pdf:pdf},
isbn = {9781450367080},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {3D visualization,hybrid visualization system,immersive analytics,user interface},
title = {{Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools}},
year = {2020}
}
@article{Lopez2016,
abstract = {We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.},
annote = {Touch-based navigation of 3D visualizatins in a combined monoscopic and stereoscopic viewing environment. Analyze the control-display space mapping between the differenct reference frames of the stereoscopic and monoscopic displays. Touch input results in users feeling "in control of the data" due to its directness. It includes parallax issues, touch through and invisble wall problems. A stereoscopic visualizations of 3D data that are controlled  via touch interaction on a monoscopic view of the data on a tablet, but also offer rich interaction through a lightweight minimalists implementation. So allow both visual immersion (steroscopy) on the one hand and immersion through interaction (Direct input) on the other hand.},
author = {L{\'{o}}pez, David and Oehlberg, Lora and Doger, Candemir and Isenberg, Tobias},
doi = {10.1109/TVCG.2015.2440233},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/TowardsAnUnderstandingOfMobileTouchNavigation.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {AR,VR,Visualization of 3D data,conceptual model of interaction,direct-touch input,expert interaction,human-computer interaction,interaction reference frame mapping,mobile displays,observational study,stereoscopic environments},
number = {5},
pages = {1616--1629},
title = {{Towards an understanding of mobile touch navigation in a stereoscopic viewing environment for 3D data exploration}},
volume = {22},
year = {2016}
}
@article{Cordeil2017a,
abstract = {We introduce ImAxes, an immersive system for exploring multivariate data using fluid, modeless interaction. The basic interface element is an embodied data axis. The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations. The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar. This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.},
annote = {ImAxes is an immersive system for exploring mulitvariate data using fluid modeless interaction wit embodied data axis.  This type of interaction goes beyond traditional mouse and keyboardinteraction and provides a greater freedom of expression and more fluid interaction. The axes can be places anywhere in space and the type of data visual is determined by their proximity and orientation with respect to each other. Build in Unity and used with HTC Vice headset. Interaction happens without menu or hotkey just by direct manipulation of the objects within the virtual environment.},
author = {Cordeil, Maxime and Cunningham, Andrew and Dwyer, Tim and Thomas, Bruce H. and Marriott, Kim},
doi = {10.1145/3126594.3126613},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/ImAxes_ImmersiveAxesAsEmbodiedAffordanceForInteractiveMultivariateDataVisualisation.pdf:pdf},
isbn = {9781450349819},
journal = {UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
keywords = {immersion,immersive analytics,immersive visualization,information visualization,multidimensional data visualization,virtual reality},
pages = {71--83},
title = {{ImAxes: Immersive axes as embodied affordances for interactive multivariate data visualisation}},
year = {2017}
}

@inproceedings {Jankowski2013,
booktitle = {Eurographics 2013 - State of the Art Reports},
title = {{A Survey of Interaction Techniques for Interactive 3D Environments}},
author = {Jankowski, Jacek and Hachet, Martin},
year = {2013},
publisher = {The Eurographics Association},
ISSN = {1017-4656},
DOI = {10.2312/conf/EG2013/stars/065-093}
}


@article{Yu2016,
abstract = {We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.},
author = {Yu, Lingyun and Efstathiou, Konstantinos and Isenberg, Petra and Isenberg, Tobias},
doi = {10.1109/TVCG.2015.2467202},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/CASTEffectiveAndEfficientUserInteractionForContext-Aeare-SelectionIn3DParticcleClouds.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Accuracy,Face,Indexes,Isosurfaces,Shape,Three-dimensional displays},
number = {1},
pages = {886--895},
pmid = {26390474},
publisher = {IEEE},
title = {{CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds}},
volume = {22},
year = {2016}
}
@article{Bach2018,
abstract = {We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user's real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.},
author = {Bach, Benjamin and Sicat, Ronell and Beyer, Johanna and Cordeil, Maxime and Pfister, Hanspeter},
doi = {10.1109/TVCG.2017.2745941},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Paper ARVR/Bach et al. - 2018 - The Hologram in My Hand How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augment.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D Interaction,Augmented Reality,Immersive Displays,User Study},
number = {1},
pages = {457--467},
pmid = {28866590},
title = {{The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?}},
volume = {24},
year = {2018}
}
@article{Donalek2015,
abstract = {Effective data visualization is a key part of the discovery process in the era of 'big data'. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional 'desktop' visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.},
author = {Donalek, Ciro and Djorgovski, S. G. and Cioc, Alex and Wang, Anwell and Zhang, Jerry and Lawler, Elizabeth and Yeh, Stacy and Mahabal, Ashish and Graham, Matthew and Drake, Andrew and Davidoff, Scott and Norris, Jeffrey S. and Longo, Giuseppe},
doi = {10.1109/BigData.2014.7004282},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/ImmersiveAndColaborativeDataVisualizationUsingVirtualRealityPlatforms.pdf:pdf},
isbn = {9781479956654},
journal = {Proceedings - 2014 IEEE International Conference on Big Data, IEEE Big Data 2014},
keywords = {Astroinformatics,Big data,Data analysis,Pattern recognition,Virtual reality,Visualization},
pages = {609--614},
publisher = {IEEE},
title = {{Immersive and collaborative data visualization using virtual reality platforms}},
year = {2015}
}
@article{Besancon2019,
author = {Besan{\c{c}}on, Lonni and Sereno, Mickael and Yu, Lingyun and Ammi, Mehdi and Isenberg, Tobias},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/HybridTouchTangibleSpatial3DDataSelection.pdf:pdf},
journal = {Computer Graphics Forum, Wiley, 2019, Eurographics Conference on Visualization (EuroVis 2019)},
number = {3},
title = {{Hybrid Touch / Tangible Spatial 3D Data Selection}},
volume = {38},
year = {2019}
}
@article{WagnerFilho2018,
abstract = {The use of novel displays and interaction resources to support immersive data visfile:///C:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/ImmersiveAndColaborativeDataVisualizationUsingVirtualRealityPlatforms.pdfualization and improve analytical reasoning is a research trend in the information visualization community. In this work, we evaluate the use of an HMD-based environment for the exploration of multidimensional data, represented in 3D scatterplots as a result of dimensionality reduction (DR). We present a new modeling for this problem, accounting for the two factors whose interplay determine the impact on the overall task performance: The difference in errors introduced by performing dimensionality reduction to 2D or 3D, and the difference in human perception errors under different visualization conditions. This two-step framework offers a simple approach to estimate the benefits of using an immersive 3D setup for a particular dataset. Here, the DR errors for a series of roll call voting datasets when using two or three dimensions are evaluated through an empirical task-based approach. The perception error and overall task performance, on the other hand, are assessed through a comparative user study with 30 participants. Results indicated that perception errors were low and similar in all approaches, resulting in overall performance benefits in both desktop and HMD-based 3D techniques. The immersive condition, however, was found to require less effort to find information and less navigation, besides providing much larger subjective perception of accuracy and engagement.},
author = {{Wagner Filho}, Jorge A. and Rey, Marina F. and Freitas, Carla M.D.S. and Nedel, Luciana},
doi = {10.1109/VR.2018.8447558},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/ImmersiveVisualisationOfAbstractInformation.pdf:pdf},
isbn = {9781538633656},
journal = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
keywords = {3D scatterplots,Immersive visualization,abstract information visualization,dimensionality reduction},
pages = {483--490},
publisher = {IEEE},
title = {{Immersive Visualization of Abstract Information: An Evaluation on Dimensionally-Reduced Data Scatterplots}},
year = {2018}
}

@article{Kwon2016,
abstract = {Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrasting very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.},
author = {Kwon, Oh Hyun and Muelder, Chris and Lee, Kyungwon and Ma, Kwan Liu},
doi = {10.1109/TVCG.2016.2520921},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/AStudyOfLayoutRenderingAndInteractionMethodsForImmersiveGraphVisualisation.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Graph Visualization,Head-Mounted Display,Immersive Environments,Virtual Reality},
number = {7},
pages = {1802--1815},
publisher = {IEEE},
title = {{A study of layout, rendering, and interaction methods for immersive graph visualization}},
volume = {22},
year = {2016}
}
@article{Zielasko2016,
abstract = {To use the full potential of immersive data analysis when wearing a head-mounted display, the user has to be able to navigate through the spatial data. We collected, developed and evaluated 5 different handsfree navigation methods that are usable while seated in the analyst's usual workplace. All methods meet the requirements of being easy to learn and inexpensive to integrate into existing workplaces. We conducted a user study with 23 participants which showed that a body leaning metaphor and an accelerometer pedal metaphor performed best within the given task.},
author = {Zielasko, Daniel and Horn, Sven and Freitag, Sebastian and Weyers, Benjamin and Kuhlen, Torsten W.},
doi = {10.1109/VR.2016.7504781},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/Evaluation_of_hands-free_HMD-based_navigation_tech.pdf:pdf},
isbn = {9781509008360},
journal = {Proceedings - IEEE Virtual Reality},
keywords = {H.5.2 [Information Interfaces and Presentation]: User Interfaces - Evaluation/methodology},
number = {March},
pages = {317--318},
title = {{Evaluation of hands-free HMD-based navigation techniques for immersive data analysis}},
volume = {2016-July},
year = {2016}
}
@article{Zielasko2017,
abstract = {In this work we describe the scenario of fully-immersive desktop VR, which serves the overall goal to seamlessly integrate with existing workflows and workplaces of data analysts and researchers, such that they can benefit from the gain in productivity when immersed in their data-spaces. Furthermore, we provide a literature review showing the status quo of techniques and methods available for realizing this scenario under the raised restrictions. Finally, we propose a concept of an analysis framework and the decisions made and the decisions still to be taken, to outline how the described scenario and the collected methods are feasible in a real use case.},
author = {Zielasko, Daniel and Weyers, Benjamin and Bellgardt, Martin and Pick, Sebastian and Meibner, Alexander and Vierjahn, Tom and Kuhlen, Torsten W.},
doi = {10.1109/WEVR.2017.7957707},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/RemainSeatedTowardsFullyImmersiveDesktopVr.pdf:pdf},
isbn = {9781538638811},
journal = {2017 IEEE 3rd Workshop on Everyday Virtual Reality, WEVR 2017},
keywords = {Artificial,H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems,augmented and virtual realities},
title = {{Remain seated: Towards fully-immersive desktop VR}},
year = {2017}
}


@article{Cordeil2017b,
abstract = {High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs) such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving abstract data. We find significant differences between the two conditions in task completion time and the physical movements of the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.},
author = {Cordeil, Maxime and Dwyer, Tim and Klein, Karsten and Laha, Bireswar and Marriott, Kim and Thomas, Bruce H.},
doi = {10.1109/TVCG.2016.2599107},
file = {:C\:/Users/leeen/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cordeil et al. - 2017 - Immersive Collaborative Analysis of Network Connectivity CAVE-style or Head-Mounted Display.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D Network,CAVE,Collaboration,Immersive Analytics,Oculus Rift},
number = {1},
pages = {441--450},
pmid = {27875160},
title = {{Immersive Collaborative Analysis of Network Connectivity: CAVE-style or Head-Mounted Display?}},
volume = {23},
year = {2017}
}
@article{Clarke2016,
abstract = {Figure 1: Screenshots of the NaturalMotion prototype being used to visualize a synthetic dataset of annual trade volumes among nations. The color mapping shown is weight-based, i.e., larger trading volumes in a given year are represented by larger blue cubes, while smaller trading volumes are smaller and colored red. a: selecting a slice in the overview cube. The rotation of the small cube in the bottom right is linked to that of the larger one, allowing the data visualization to be rotated without being blocked from view. b: viewing a slice in the single slice view. ABSTRACT We are investigating new ways for people to explore time-evolving graph data represented as the recently-introduced Matrix Cube, through natural gestures in a 3D environment. In the cube, one axis is time, and the other two axes comprise an adjacency matrix of a graph. We want to understand how recent advances in virtual reality technologies may help the user more naturally explore the dimensionality and richness of this 3D visualization, enabling them to more effectively gain insights into relationships and anomalies in the data. We use a Leap Motion controller to capture the user's hand gestures that manipulate the cube into its many possible projections. We prototyped this synergy of data visualization and virtual reality on a time-evolving graph of annual trading volumes among major countries from 1995 through 2010. Please see the accompanying video for a demo of our prototype. 1},
author = {Clarke, Samuel and Dass, Nathan and Horng, Duen and Chau, Polo},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/NaturalMotion_ExploringGestureControlForVisualizinhTimeEvolvongGraphs.pdf:pdf},
journal = {IEEE Visualization Poster Proceedings},
pages = {3--4},
title = {{NaturalMotion : Exploring Gesture Controls for Visualizing Time-Evolving Graphs}},
year = {2016}
}
@article{Nagao2017,
abstract = {Validation and exploration of the data generated by large-scale scientific simulations rely on sophisticated visualization and analysis tasks. With the advancement of supercomputing, the growing scale and complexity of the data make some of these tasks challenging, which demands new hardware and software solutions. We believe it is possible to address some of the challenges by utilizing the increasingly affordable see-through head mounted display (H-MD) devices together with a low-cost tiled HDTV display. With the tiled display to provide a high-resolution overview of the data, the user can freely choose a small area to explore and analyze using a see-through HMD in stereoscopic 3D with gesture input. During such local exploration and detail data analysis, the user can apply a newly derived visualization parameter setting to the large tiled display for a new overview. In this way, computational costs become more manageable because realtime rendering and response are only required to cover a small screen space and a subset of the data. In our current study, we focus on supporting immersive isosurface and streamline visualization and analysis of 3D flow field data. In this workshop paper, we present our preliminary design and results, and we also discuss our further development and evaluation plan.},
author = {Nagao, Ken and Ye, Yucong and Wang, Chuan and Fujishiro, Issei and Ma, Kwan Liu},
doi = {10.1109/IMMERSIVE.2016.7932374},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/gesture/EnablingINteractiveScientificDataVisualisationAndAnalysisWithSeeThroughHMDsAndALArgeTiledDisplay.pdf:pdf},
isbn = {9781509008346},
journal = {2016 Workshop on Immersive Analytics, IA 2016},
keywords = {Augmented and Virtual Realities,H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial,H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input Devices and Strategies,Interaction Styles},
pages = {1--6},
publisher = {IEEE},
title = {{Enabling interactive scientific data visualization and analysis with see-through hmds and a large tiled display}},
year = {2017}
}

@article{Bhowmick2021,
abstract = {Recent years have seen incredible growth of Virtual Reality (VR) interfaces. In the area of data visualization and analytics, VR applications offers immense opportunities to interact, modify and explore 3D data in an interactive manner, which help in establishing new trends and patterns. In this regard, object selection is of primary importance. It is the fundamental and initial task in any immersive VR. However, the current literature is limited in investigating the effectiveness of object selection techniques in different VEs including dense and occluded dense VE, varied object sizes, proximity, and distances in the area of Immersive Analytics. In this paper, I present my ongoing PhD research to explore controller-less gestures for nail-size object selection on HMD-VR interface for dense and occluded dense VE. I describe the experiments, the findings and my future studies. I believe the outcome of these experiments in the form of guidelines or framework will enable researchers to design controller-less body gestures for object selection task.},
author = {Bhowmick, Shimmila},
doi = {10.1109/vrw52623.2021.00239},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/gesture/ExploringBodyGesturesForSmallObjectSelectinInDenceEnvironemtinHMDVRForDataVisaulisationApplications.pdf:pdf},
isbn = {9780738113678},
keywords = {dense and occluded virtual,environment,hci,hci design and evaluation,human computer,human-centered computing,immersive analytics,index terms,interaction,methods,nail-size objects,object selection,user,virtual reality},
pages = {713--714},
title = {{Exploring Body Gestures for Small Object Selection in Dense Environment in HMD VR for Data Visualization Applications}},
year = {2021}
}
@article{Theart2017,
abstract = {Background: Confocal microscopes deliver detailed three-dimensional data and are instrumental in biological analysis and research. Usually, this three-dimensional data is rendered as a projection onto a two-dimensional display. We describe a system for rendering such data using a modern virtual reality (VR) headset. Sample manipulation is possible by fully-immersive hand-tracking and also by means of a conventional gamepad. We apply this system to the specific task of colocalization analysis, an important analysis tool in biological microscopy. We evaluate our system by means of a set of user trials. Results: The user trials show that, despite inaccuracies which still plague the hand tracking, this is the most productive and intuitive interface. The inaccuracies nevertheless lead to a perception among users that productivity is low, resulting in a subjective preference for the gamepad. Fully-immersive manipulation was shown to be particularly effective when defining a region of interest (ROI) for colocalization analysis. Conclusions: Virtual reality offers an attractive and powerful means of visualization for microscopy data. Fully immersive interfaces using hand tracking show the highest levels of intuitiveness and consequent productivity. However, current inaccuracies in hand tracking performance still lead to a disproportionately critical user perception.},
author = {Theart, Rensu P. and Loos, Ben and Niesler, Thomas R.},
doi = {10.1186/s12859-016-1446-2},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/VirtualRealityAssistedMicroscopyDataVisualisationAndColocalizationAnalysis.pdf:pdf},
isbn = {1285901614},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {3D Microscopic reconstruction,Colocalization analysis,Confocal microscopy visualization,Hand tracking,Region of interest selection,Virtual reality,Volume rendering},
number = {Suppl 2},
pages = {1--16},
pmid = {28251867},
title = {{Virtual reality assisted microscopy data visualization and colocalization analysis}},
volume = {18},
year = {2017}
}
@article{Issartel2014,
abstract = {Manipulating slice planes is an important task for exploring volumetric datasets. Since this task is inherently 3D, it is difficult to accomplish with standard 2D input devices. Alternative interaction techniques have been proposed for direct and natural 3D manipulation of slice planes. However, they also require bulky and dedicated hardware, making them inconvenient for everyday work. To address this issue, we adapted two of these techniques for use in a portable and self-contained handheld AR environment. The first is based on a tangible slicing tool, and the other is based on a spatially aware display. In this paper, we describe our design choices and the technical challenges encountered in this implementation. We then present the results, both objective and subjective, from an evaluation of the two slicing techniques. Our study provides new insight into the usability of these techniques in a handheld AR setting. {\textcopyright} 2014 IEEE.},
author = {Issartel, Paul and Gueniat, Florimond and Ammi, Mehdi},
doi = {10.1109/3DUI.2014.6798839},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/Slicing Techniques for Handheld Augmented Reality.pdf:pdf},
journal = {IEEE Symposium on 3D User Interfaces 2014, 3DUI 2014 - Proceedings},
keywords = {3D interaction,Augmented reality,Scientific visualization,tangible user interface},
number = {March},
pages = {39--42},
title = {{Slicing techniques for handheld augmented reality}},
year = {2014}
}
@article{Filho2020,
abstract = {A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.},
archivePrefix = {arXiv},
arxivId = {1908.00580},
author = {Filho, Jorge A.Wagner and Stuerzlinger, Wolfgang and Nedel, Luciana},
doi = {10.1109/TVCG.2019.2934415},
eprint = {1908.00580},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/not yet reviewed_IEEE/Evaluating an Immserive Space-TimeCubeGeoVisualisationForIntuitiveTrajectoryDataExploration.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Immersive analytics,Space-time cube,Trajectory visualization},
number = {1},
pages = {514--524},
pmid = {31581085},
title = {{Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration}},
volume = {26},
year = {2020}
}

@article{Butcher2020,
abstract = {We present VRIA, a Web-based framework for creating Immersive Analytics (IA) experiences in Virtual Reality. VRIA is built upon WebVR, A-Frame, React and D3.js, and offers a visualization creation workflow which enables users, of different levels of expertise, to rapidly develop Immersive Analytics experiences for the Web. The use of these open-standards Web-based technologies allows us to implement VR experiences in a browser and offers strong synergies with popular visualization libraries, through the HTML Document Object Model (DOM). This makes VRIA ubiquitous and platform-independent. Moreover, by using WebVR's progressive enhancement, the experiences VRIA creates are accessible on a plethora of devices. We elaborate on our motivation for focusing on open-standards Web technologies, present the VRIA creation workflow and detail the underlying mechanics of our framework. We also report on techniques and optimizations necessary for implementing Immersive Analytics experiences on the Web, discuss scalability implications of our framework, and present a series of use case applications to demonstrate the various features of VRIA. Finally, we discuss current limitations of our framework, the lessons learned from its development, and outline further extensions.},
author = {Butcher, Peter W. S. and John, Nigel W. and Ritsos, Panagiotis D.},
doi = {10.1109/tvcg.2020.2965109},
file = {:C\:/Users/leeen/Documents/FH_MMT/4.Semester/seminararbeit/Quellen/VRIAA Web-Based Framework for Creating Immersive Analytics Experiences.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {7},
pages = {3213--3225},
pmid = {31944959},
title = {{VRIA: A Web-Based Framework for Creating Immersive Analytics Experiences}},
volume = {27},
year = {2020}
}
@inproceedings{Buschel2021,
   author = {Wolfgang B\"{u}schel and Anke Lehmann and Raimund Dachselt},
   title = {MIRIA: A Mixed Reality Toolkit for the In-Situ Visualization and Analysis of Spatio-Temporal Interaction Data},
   booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
   series = {CHI '21},
   year = {2021},
   month = {5},
   isbn = {978-1-4503-8096-6/21/05},
   location = {Yokohama, Japan},
   numpages = {15},
   doi = {10.1145/3411764.3445651},
   publisher = {ACM},
   address = {New York, NY, USA},
   keywords = {interaction analysis, immersive analytics, in-situ analysis, in-situ visualization, augmented reality, human-computer interaction, visualization}
}

@inproceedings{Langner2021,
   author = {Ricardo Langner and Marc Satkowski and Wolfgang B\"{u}schel and Raimund Dachselt},
   title = {MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis},
   booktitle = {Proceedings of the 2021 ACM Conference on Human Factors in Computing Systems},
   year = {2021},
   month = {5},
   isbn = {978-1-4503-8096-6/21/05},
   location = {Yokohama, Japan},
   numpages = {17},
   doi = {10.1145/3411764.3445593},
   publisher = {ACM},
   address = {New York, NY, USA},
   keywords = {data visualization, mobile devices, head-mounted augmented reality, cross-device interaction, data analysis, mobile data visualization, augmented displays, immersive analytics}
}
